{"cells":[{"cell_type":"markdown","metadata":{"id":"tmPdeNjs-usF"},"source":["# Clase 2\n","En esta clase vamos a poner en practica los conocimiento vistos en la primera clase.\n","\n","Vamos a utilizar un dataset bastante conocido: Titanic, que muestra datos de los pasajeros del crucero. El objetivo es entrenar un clasificador binario que, a partir de los datos de los pasajeros, clasifique correctamente su supervivencia.\n","\n","Comencemos por descargarlo en la siguiente celda:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1850,"status":"ok","timestamp":1695824458573,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"8hFEhzXLOt_1","outputId":"363b578b-66da-4037-d94e-e69495a97dbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-09-27 14:20:56--  https://eva.fing.edu.uy/pluginfile.php/255092/mod_folder/content/0/titanic.txt\n","Resolving eva.fing.edu.uy (eva.fing.edu.uy)... 164.73.32.9\n","Connecting to eva.fing.edu.uy (eva.fing.edu.uy)|164.73.32.9|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 116946 (114K) [text/plain]\n","Saving to: ‘titanic.txt.1’\n","\n","titanic.txt.1       100%[===================>] 114.21K   223KB/s    in 0.5s    \n","\n","2023-09-27 14:20:58 (223 KB/s) - ‘titanic.txt.1’ saved [116946/116946]\n","\n"]}],"source":["! wget https://eva.fing.edu.uy/pluginfile.php/255092/mod_folder/content/0/titanic.txt"]},{"cell_type":"markdown","metadata":{"id":"XdsYHtBOrFrq"},"source":["La siguiente celda es para verificar que la descarga fue exitosa. En caso de tener problemas, pueden hacerlo manualmente desde el EVA del curso, en el material de la clase 2."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"lMgBki_gkLpa","executionInfo":{"status":"ok","timestamp":1695824458574,"user_tz":180,"elapsed":3,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["import os\n","assert os.path.isfile(\"titanic.txt\"), \"No se descargo el archivo! Verificar la ejecucion de la celda anterior.\""]},{"cell_type":"markdown","metadata":{"id":"n_YDGSfw-usI"},"source":["Luego vamos a importar algunas librerias genericas.\n","\n","_Nota: Siempre es buena idea importar las librerías genéricas al principio._"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"F2AJS_Oz-usK","executionInfo":{"status":"ok","timestamp":1695824458574,"user_tz":180,"elapsed":2,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["import sklearn as sk\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","RANDOM_STATE=0"]},{"cell_type":"markdown","metadata":{"id":"iJBU6x7--usL"},"source":["## Importación y procesamiento de datos\n","En todo proyecto de aprendizaje automático es fundamental manejar el conjunto de datos.\n","Es importante tener una noción del conjunto de datos, para saber, entre otras cosas:\n","* Qué atributos son numéricos, y cuáles son categóricos.\n","* Si hace falta normalizar los atributos (depende del algoritmo a utilizar también)\n","* La presencia de atributos faltantes."]},{"cell_type":"markdown","metadata":{"id":"wti5JaJH-usM"},"source":["### Importación\n","Primero importamos los datos.  \n","Al observar el archivo, pueden identificar columnas e instancias que no sean necesarias?"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1695824459038,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"GWBSuIrh-usN","outputId":"b93dad9b-013d-4718-97c8-dccf637a87c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mostramos para cada columna, el porcentaje de datos faltantes:\n","\n","pclass       0.000000\n","survived     0.000000\n","age         51.789794\n","embarked    37.471439\n","room        94.135567\n","ticket      94.744859\n","boat        73.571973\n","sex          0.000000\n","dtype: float64\n"]}],"source":["import os\n","import pandas as pd\n","\n","assert os.path.isfile('titanic.txt'), 'No se encontró el archivo titanic.txt; asegurate de haberlo cargado'\n","\n","# leemos el dataset utilizando Pandas\n","data = pd.read_csv('titanic.txt')\n","# eliminamos la columna row.names que solo tiene el nuero de fila\n","# tambien vamos a eliminar los atributos 'name' y 'home.dest'\n","# ya que contienen texto libre, y aun no hemos visto como tratar con ellos\n","data.drop(['row.names', 'name', 'home.dest'], axis=1, inplace=True)\n","\n","print('Mostramos para cada columna, el porcentaje de datos faltantes:\\n')\n","print(data.isnull().mean()*100)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"LyMW796DWxuf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":9,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"18f62706-aae0-4094-eb10-e11accd15767"},"outputs":[{"output_type":"stream","name":"stdout","text":["X tiene forma (1313, 4)\n","y tiene forma (1313,)\n"]}],"source":["# === Su codigo empieza acá ===\n","# Modificar la lista columns de manera que contenga solo aquellos atributos\n","# que querramos a usar. Notar que actualmente tiene todos los atributos\n","# borren aquellos que crean inutiles\n","\n","columns = ['pclass', 'embarked', 'sex', 'age'] # elimino boat, room, ticket\n","\n","# === Su codigo termina acá ===\n","\n","# Nos quedamos con los datos como numpy array\n","X = data[columns].values\n","y = data['survived'].values\n","\n","print('X tiene forma', X.shape)\n","print('y tiene forma', y.shape)"]},{"cell_type":"markdown","metadata":{"id":"W3RjY6eBTR0N"},"source":["En la siguiente celda, interesa contar cuantos casos hay en cada clase, para elegir una metrica apropiada:\n","\n","_Pista: hay varias formas de hacerlo, por ejemplo, pueden usar la función [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html)_"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"EVMcTboQef94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":8,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"7dd6592f-3291-4dfa-e72f-e66c4da0ff77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Valores únicos de columna \"survived\" - value_counts()\n","0    864\n","1    449\n","Name: survived, dtype: int64\n","\n","Valores únicos de columna \"survived\" - np.unique\n","Survived: 0 - Cantidad = 864\n","Survived: 1 - Cantidad = 449\n"]}],"source":["# === Su código empieza acá ===\n","# value_counts()\n","print ('Valores únicos de columna \"survived\" - value_counts()')\n","print(f\"{data['survived'].value_counts()}\\n\")\n","\n","# np.unique\n","print ('Valores únicos de columna \"survived\" - np.unique')\n","for cat,count in zip(np.unique(y, return_counts = True)[0], np.unique(y, return_counts = True)[1]):\n","    print (f'Survived: {cat} - Cantidad = {count}')\n","# === Su código termina acá ==="]},{"cell_type":"markdown","metadata":{"id":"jAs6wOanc1yx"},"source":["Vamos a visualizar algunos ejemplos al azar, utilizando la funcion `show_some_samples`:"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"OFlsWmESc9Eg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":7,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"8c941b45-bdef-4273-f6e7-a45923e80ceb"},"outputs":[{"output_type":"stream","name":"stdout","text":["==== idx   1015 :: target = 0 ====\n","\tpclass: 3rd\n","\tembarked: nan\n","\tsex: male\n","\tage: nan\n","==== idx   1208 :: target = 0 ====\n","\tpclass: 3rd\n","\tembarked: nan\n","\tsex: male\n","\tage: nan\n","==== idx   1280 :: target = 0 ====\n","\tpclass: 3rd\n","\tembarked: nan\n","\tsex: male\n","\tage: nan\n"]}],"source":["def show_some_samples(X, y, columns=columns, n_samples=3, seed=None):\n","  \"\"\"\n","  show random instances from X with its label in y.\n","  X: dataset to sample, as a numpy array of shape (n_samples, n_features)\n","  y: labels, as a numpy array of shape (n_samples,)\n","  columns: list of string with the name of each column, so len(columns) == n_features\n","  n_samples: number of samples to show\n","  seed: seed to set before choosing examples\n","  \"\"\"\n","  if seed is not None:\n","    np.random.seed(seed=seed)\n","\n","  idx = np.random.choice(len(X), n_samples)\n","\n","  for i, (x, t) in enumerate(zip(X[idx], y[idx])):\n","    print(f'==== idx {idx[i]:6d} :: target = {t} ====')\n","    for feat_name, feat_value in zip(columns, x):\n","      print(f'\\t{feat_name}: {feat_value}')\n","\n","# aca esta la invocacion, no tienen que cambiar nada\n","show_some_samples(X, y)"]},{"cell_type":"markdown","metadata":{"id":"cZ9ux4FAYxSq"},"source":["# Pregunta A\n","\n","Qué debería hacer primero:  \n","\n","\n","1.   Rellenar los datos faltantes con la politica elegida (por ejemplo, el más frecuente)\n","2.   Partir el dataset en entrenamiento y test\n","\n","**Respuesta:**\n","En primer lugar se debería separar el dataset en entrenamiento y test, y luego rellenar los datos faltantes de ambos sets con el valor más frecuente del set de entrenamiento. Si se rellenaran los datos faltantes utilizando todo el dataset, se estaría realizando el preprocesamiento del set de entrenamiento utilizando datos que van a formar parte del test set. Esto podría llevar a la sobreestimación de la performance del modelo, ya que se estaría evaluando el modelo con datos que fueron utilizados para la preparación de los datos de entrenamiento. En cambio, realizar la partición del dataset antes de hacer la imputación permite emular una situación lo más cercana posible a una situación real y asegura la consistencia entre el set de entrenamiento y de test, ya que se está imputando el valor más frecuente de los datos de entrenamiento, y estos datos serán los únicos disponibles al momento de realizar una predicción sobre datos nuevos cuando el modelo esté en producción.   \n","\n","*Nota* Ajuste el orden de las siguientes celdas de acuerdo a lo que crea más conveniente.\n"]},{"cell_type":"markdown","metadata":{"id":"ofvGN3TGX3g_"},"source":["# 1: Separar train y test\n","En la siguiente celda, utilizar la funcion `sklearn.model_selection.train_test_split` para separar el dataset en entrenamiento y test. Vamos a tomar un 30% para test, y utilizar como semilla la constante `RANDOM_STATE`:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_uP92xlyYQ6_","executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":6,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# === Su código empieza acá ===\n","# X_train, X_test, y_train, y_test = # completar la invocación\n","x = data[columns]\n","y = data['survived']\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = RANDOM_STATE)\n","# === Su código termina acá ===\n","\n","# El siguiente codigo es un chequeo automatico de que todo va bien\n","# Si no salta ningun error, es porque esta todo Ok\n","assert len(X_train) == len(y_train), f'X_train e y_train deberian tener la misma cantidad de elementos: {len(X_train)} != {len(y_train)}'\n","assert len(X_test) == len(y_test), f'X_test e y_test deberian tener la misma cantidad de elementos: {len(X_test)} != {len(y_test)}'\n","assert X_train.shape[1] == X_test.shape[1], f'X_train y X_test deberian tener los mismos atributos: {X_test.shape[1]} != {X_test.shape[1]}'\n","assert len(X) * 0.28 < len(X_test) < len(X) * 0.32, 'Verificar que el test sea 30%'"]},{"cell_type":"markdown","metadata":{"id":"a3hSa8VZZ1V3"},"source":["# 2: Imputar datos faltantes\n","Utilizar la clase `sklearn.impute.SimpleImputer` para rellenar los atributos faltantes.\n","\n","Probar la estrategia `mean` y `most_frequent`."]},{"cell_type":"markdown","metadata":{"id":"gSTWVPAqt9eX"},"source":["# Pregunta B\n","\n","**Cuál crees que es más adecuada? Justifique**"]},{"cell_type":"markdown","metadata":{"id":"dP18m5es_lWj"},"source":["**Respuesta:**\n","Dado que, de las cuatro columnas que serán utilizadas, tres de ellas corresponden a variables categóricas y una corresponde a una variable numérica, es más adecuado realizar la imputación con el valor más frecuente (ya que no es posible calcular el promedio de valores de una variable categórica)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"W2w2mlnfCXAG","executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":6,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","# === Su código empieza acá ===\n","# definir el imputer y entrenarlo\n","imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n","imputer.fit(X_train)\n","# === Su código termina acá ===\n","\n","X_train_fill = imputer.transform(X_train)\n","X_test_fill = imputer.transform(X_test)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdOj5ZCd_lWj","executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":6,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"0759c2b0-52ed-4fc5-e36f-3d0653601d16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Valores nulos antes de imputación\n","Entrenamiento:\n","pclass        0\n","embarked    334\n","sex           0\n","age         471\n","dtype: int64\n","\n","Testeo:\n","pclass        0\n","embarked    158\n","sex           0\n","age         209\n","dtype: int64\n","\n","Valores nulos luego de imputación\n","Entrenamiento: 0\n","Testeo: 0\n"]}],"source":["# Chequeo que ya no existan valores nulos\n","\n","# Antes de imputación\n","print(f'Valores nulos antes de imputación')\n","print(f'Entrenamiento:\\n{X_train.isna().sum()}\\n')\n","print(f'Testeo:\\n{X_test.isna().sum()}\\n')\n","\n","# Luego de imputación\n","print(f'Valores nulos luego de imputación')\n","print(f'Entrenamiento: {(X_train_fill == np.nan).sum()}')\n","print(f'Testeo: {(X_test_fill == np.nan).sum()}')"]},{"cell_type":"markdown","metadata":{"id":"8BERiOyhk6o8"},"source":["En la siguiente celda vamos a ver como son los contenidos de cada atributo: cuantos hay de cada tipo.\n","\n","El objetivo de su ejecución es ayudarnos a decidir cuáles son y cómo vamos a codificar los atributos categóricos."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"saYImSVLjPcY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":5,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"bc070052-e551-4118-fa10-a04fdd7ed44d"},"outputs":[{"output_type":"stream","name":"stdout","text":["===0: pclass===\n","\t1st: 231 - 25.14 %\n","\t2nd: 189 - 20.57 %\n","\t3rd: 499 - 54.30 %\n","===1: embarked===\n","\tCherbourg: 150 - 16.32 %\n","\tQueenstown: 30 -  3.26 %\n","\tSouthampton: 739 - 80.41 %\n","===2: sex===\n","\tfemale: 323 - 35.15 %\n","\tmale: 596 - 64.85 %\n","===3: age===\n","\t0.1667: 1 -  0.11 %\n","\t0.3333: 1 -  0.11 %\n","\t0.8333: 2 -  0.22 %\n","\t0.9167: 1 -  0.11 %\n","\t1.0: 1 -  0.11 %\n","\t2.0: 3 -  0.33 %\n","\t3.0: 4 -  0.44 %\n","\t4.0: 4 -  0.44 %\n","\t5.0: 1 -  0.11 %\n","\t6.0: 4 -  0.44 %\n","\t7.0: 1 -  0.11 %\n","\t8.0: 3 -  0.33 %\n","\t9.0: 7 -  0.76 %\n","\t10.0: 1 -  0.11 %\n","\t11.0: 1 -  0.11 %\n","\t12.0: 2 -  0.22 %\n","\t13.0: 3 -  0.33 %\n","\t14.0: 2 -  0.22 %\n","\t15.0: 3 -  0.33 %\n","\t16.0: 9 -  0.98 %\n","\t17.0: 6 -  0.65 %\n","\t18.0: 14 -  1.52 %\n","\t19.0: 13 -  1.41 %\n","\t20.0: 9 -  0.98 %\n","\t21.0: 17 -  1.85 %\n","\t22.0: 15 -  1.63 %\n","\t23.0: 16 -  1.74 %\n","\t24.0: 12 -  1.31 %\n","\t25.0: 13 -  1.41 %\n","\t26.0: 490 - 53.32 %\n","\t27.0: 13 -  1.41 %\n","\t28.0: 15 -  1.63 %\n","\t29.0: 9 -  0.98 %\n","\t30.0: 18 -  1.96 %\n","\t31.0: 8 -  0.87 %\n","\t32.0: 14 -  1.52 %\n","\t33.0: 10 -  1.09 %\n","\t34.0: 8 -  0.87 %\n","\t35.0: 13 -  1.41 %\n","\t36.0: 18 -  1.96 %\n","\t37.0: 4 -  0.44 %\n","\t38.0: 8 -  0.87 %\n","\t39.0: 7 -  0.76 %\n","\t40.0: 10 -  1.09 %\n","\t41.0: 7 -  0.76 %\n","\t42.0: 6 -  0.65 %\n","\t43.0: 4 -  0.44 %\n","\t44.0: 4 -  0.44 %\n","\t45.0: 9 -  0.98 %\n","\t46.0: 6 -  0.65 %\n","\t47.0: 5 -  0.54 %\n","\t48.0: 10 -  1.09 %\n","\t49.0: 8 -  0.87 %\n","\t50.0: 8 -  0.87 %\n","\t52.0: 4 -  0.44 %\n","\t53.0: 2 -  0.22 %\n","\t54.0: 4 -  0.44 %\n","\t55.0: 3 -  0.33 %\n","\t56.0: 2 -  0.22 %\n","\t57.0: 1 -  0.11 %\n","\t58.0: 4 -  0.44 %\n","\t59.0: 3 -  0.33 %\n","\t60.0: 4 -  0.44 %\n","\t61.0: 1 -  0.11 %\n","\t62.0: 1 -  0.11 %\n","\t63.0: 2 -  0.22 %\n","\t64.0: 3 -  0.33 %\n","\t65.0: 1 -  0.11 %\n","\t69.0: 1 -  0.11 %\n","\t70.0: 1 -  0.11 %\n","\t71.0: 1 -  0.11 %\n"]}],"source":["# Iteramos sobre cada una de las columnas\n","for idx, clm in enumerate(columns):\n","  print(f'==={idx}: {clm}===')\n","  # Para cada columna, contamos la cantidad de valores unicos que hay\n","  unq, cnt = np.unique(X_train_fill[:, idx], return_counts=True)\n","  for u, c in zip(unq, cnt):\n","    # mostramos cada valor unico, con la cantidad que hay,\n","    # y qué porcentaje representa del dataset\n","    print(f'\\t{u}: {c} - {100*c/cnt.sum():5.2f} %')"]},{"cell_type":"markdown","metadata":{"id":"FuS1iypXbAqC"},"source":["# 3: Codificar atributos categóricos\n","El siguente paso va a ser codificar los atributos categóricos.\n","\n","En clase, mencionamos principalmente dos estrategias: `sklearn.preprocessing.OrdinalEncoder` y `sklearn.preprocessing.OneHotEncoder`. Utilice la (o las) que crea más conveniente.\n","\n","**PISTA**\n","\n","Hasta el momento, tenemos un maximo de 8 atributos. Sin embargo, **no todos ellos son categoricos**, por lo que en realidad solo necesito transformar alguno de ellos.\n","\n","Ademas, para cada uno, podria necesitar una codificacion distinta.\n","\n","Para esto, nos vamos a ayudar del transformador `sklearn.compose.ColumnTransformer`, que permite aplicar un transformador diferente a cada atributo (columna), y cuyo funcionamiento es el siguiente:\n","\n","```python\n","from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","\n","transformers = [\n","        # ('nombre_arbitrario', Transformador, [indices])\n","        (\"trnf1\", OrdinalEncoder(), [0]),\n","        (\"trnf2\", OneHotEncoder(), [1, 2]),\n","        (\"scaler\", MinMaxScaler(), [3])\n","     ]\n","\n","ct = ColumnTransformer(transformers, remainder='passthrough')\n","\n","ct.fit(X)\n","X_trans = ct.transform(X)\n","```\n","\n","El `ColumnTransformer` recibe una lista de transformadores a aplicar, indicando a qué columna aplicarlo, y se debe especificar qué hacer con el resto de las columnas. En el ejemplo, `reminder='passthrough'` quiere decir que los valores se pasan de largo sin ninguna modificación."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1695824459039,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"fLx6zeFDa_vv"},"outputs":[],"source":["from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","\n","# === Su código empieza acá ===\n","# sugerencia: verificar el shape antes y despues de las transformaciones\n","# para asegurar que esta todo coherente\n","# definir el column transformer (ct) y entrenarlo\n","\n","transformers = [\n","        (\"ord\", OrdinalEncoder(), [0]),\n","        (\"one_hot\", OneHotEncoder(), [1, 2]),\n","        (\"scaler\", MinMaxScaler(), [3])\n","     ]\n","\n","ct = ColumnTransformer(transformers, remainder = 'passthrough')\n","\n","# Entrenamiento\n","ct.fit(X_train_fill)\n","\n","# === Su código termina acá ===\n","X_train_fill_num = ct.transform(X_train_fill)"]},{"cell_type":"markdown","metadata":{"id":"faScon6DUlmZ"},"source":["Verificar el shape, que tenga sentido"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"BpuQJwZCGGNq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":4,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"cf2436e0-426f-4f5b-fd59-e9e31a2a2305"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set shape before transformation: (919, 4)\n","Train set shape after transformation: (919, 7)\n","--------------------------------------------------------------------------------\n","Variables luego de transformación:\n","Var 0: \t ord__x0 \t type: float64\n","Var 1: \t one_hot__x1_Cherbourg \t type: float64\n","Var 2: \t one_hot__x1_Queenstown \t type: float64\n","Var 3: \t one_hot__x1_Southampton \t type: float64\n","Var 4: \t one_hot__x2_female \t type: float64\n","Var 5: \t one_hot__x2_male \t type: float64\n","Var 6: \t scaler__x3 \t type: float64\n","--------------------------------------------------------------------------------\n"]}],"source":["print (f'Train set shape before transformation: {X_train_fill.shape}')\n","print (f'Train set shape after transformation: {X_train_fill_num.shape}')\n","print ('-'*80)\n","print (f'Variables luego de transformación:')\n","\n","for i,var in enumerate (ct.get_feature_names_out()):\n","    print (f'Var {i}: \\t {var} \\t type: {type(X_train_fill_num[:,i][0]).__name__}')\n","\n","print ('-'*80)"]},{"cell_type":"markdown","metadata":{"id":"URsO7vYjufSe"},"source":["# Pregunta C\n","Cuantas columnas tiene, por qué y qué representa cada una?\n","\n","_Nota: no es estrictamente necesario saber cada columna con qué se corresponde (es decir: que hay en la columna 0? que hay en la columna 1? ... no es necesario responder a ese nivel)._\n","\n","_Se espera que si en este punto tienen, por ejemplo, 13 columnas, explique cuáles son y cómo llegaste a ellas._"]},{"cell_type":"markdown","metadata":{"id":"8sXvjkQs_lWk"},"source":["**Respuesta:**\n","Se tienen 7 columnas. Dado que se usó OneHotEncoder para las variables \"embarked\" y \"sex\", y dichas variables tienen 3 y 2 valores únicos respectivamente, se generan 3 columnas para cada valor de \"embarked\" y 2 para cada valor de \"sex\". Luego, las 2 columnas restantes corresponden a la codificación ordinal de \"pclass\" y a la normalización de \"age\"."]},{"cell_type":"markdown","metadata":{"id":"s8avUgpLEFqS"},"source":["\n","# 4: Seleccionar atributos\n","Utilizar alguna de las estrategias vistas en clase para quedarnos con 5 atributos."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"arZObzwgEFV0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":3,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"dd5286f3-a5a6-4244-e327-c974bcfe4b2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Antes de selección de atributos: (919, 7)\n","Luego de selección de atributos: (919, 5)\n"]}],"source":["from sklearn.feature_selection import RFE, SelectKBest, chi2, SequentialFeatureSelector, f_classif\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# === Su código empieza acá ===\n","# definir el feature selector (fs) y entrenarlo\n","\n","print (f'Antes de selección de atributos: {X_train_fill_num.shape}')\n","fs = SelectKBest(f_classif, k = 5)\n","fs.fit(X_train_fill_num, y_train)\n","\n","# === Su código termina acá ===\n","X_train_fill_num_selected = fs.transform(X_train_fill_num)\n","print (f'Luego de selección de atributos: {X_train_fill_num_selected.shape}')"]},{"cell_type":"markdown","metadata":{"id":"ttbi5CSYi10k"},"source":["# 5: Entrenar un clasificador\n","En el siguiente paso, vamos a entrenar un clasificador [`sklearn.tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) con los datos siguiendo todas las transformaciones que seguimos hasta acá.\n","\n","Utilizar `sklearn.model_selection.GridSearchCV` o `sklearn.model_selection.RandomizedSearchCV` para seleccionar los mejores parametros.\n","\n","Utilizar validacion cruzada con 10 particiones. Utilizar una metrica acorde al problema."]},{"cell_type":"markdown","metadata":{"id":"JSTN30Lwwqiq"},"source":["# Pregunta D\n"," - Cuál es la **menor** cantidad de particiones que puedo usar?\n"," - Cuál es la **mayor** cantidad de particiones que puedo usar?\n"," - Qué pasa en cada uno de estos extremos?\n"," - Qué métrica vas a usar? **Justifique brevemente**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tlybO_74_lWk"},"source":["#### **Respuestas**\n","\n","1. La **menor** cantidad de particiones que se puede usar es 1, en cuyo caso no se estaría utilizando validación cruzada propiamente dicho. Utilizar una única partición implicaría partir el dataset de entrenamiento una única vez, obteniendo un set de entrenamiento con el cual se entrenará el clasificador y un set de validación con el cual se lo evaluará. Dado que se tiene una única partición, no habrá más etapas de entrenamiento ni se promediará ningún resultado, ya que el único resultado de performance del modelo será el obtenido al evaluarlo con el set de validación. La mayor desventaja de este método es la variabilidad de los resultados, ya que distintas posibles particiones pueden llevar a resultados distintos.\n","\n","2. La **mayor** cantidad de particiones que se puede usar es **n**, siendo **n** la cantidad de observaciones del set de entrenamiento (en este caso, X_train). En este caso, el cual es denominado Leave One Out Cross Validation (LOOCV), se entrenan **n** modelos distintos. Cada modelo es entrenado con **n-1** datos y es evaluado con el dato restante. Luego, se promedian los **n** resultados para obtener el resultado final. Este método tiene muy poco sesgo debido a que se utilizan todos los datos (excepto  uno) para entrenar el modelo, pero tiene mucha varianza ya que todos los modelos construidos están muy correlacionados. Asimismo, el costo computacional puede ser alto debido a que se debe entrenar **n** modelos distintos.\n","\n","3. Como se vio más arriba, las clases están desbalanceadas, ya que 449 pasajeros sobrevivieron y 864 no sobrevivieron. En virtud de esto, se opta por no utilizar la métrica de Accuracy, ya que la misma puede tomar valores altos aún cuando la clase minoritaria tenga un error de clasificación relativamente alto. Asimismo, a priori no parece que el costo de clasificar erróneamente alguna de las clases sea mayor que el costo de clasificar erróneamente la otra clase, es decir, el costo de los falsos positivos y falsos negativos es considerado como igual. Luego, dado que no se busca penalizar ningún tipo de error sobre el otro, se opta por la utilización de la métrica **f1-score** para la evaluación del modelo. Esta métrica es igual a la media armónica de Recall y Precision, y representa una ponderación de ambas, lo que permite tener una evaluación indirecta de los errores de clasificación de ambas clases. Si, por ejemplo, el modelo clasificara la gran mayoría de los datos en una única clase, Recall o Precision podrían tener un valor muy cercano a 1 de manera individual, pero no de manera simultánea, lo que se verá reflejado en el f1-score. Cabe destacar, igualmente, que se utiliza esta métrica para la comparación de modelos y elección de hiperparámetros, pero se utilizan las cuatro ya mencionadas para la evaluación del modelo final elegido.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"PS_vjucG_lWk","executionInfo":{"status":"ok","timestamp":1695824459039,"user_tz":180,"elapsed":2,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["# Definición paramétrica de métrica a utilizar\n","scoring = 'f1'"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":865,"status":"ok","timestamp":1695824459902,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"O5iE2CTEk720","outputId":"46c8c1ee-7b14-4f64-a859-724b5f2fbf75"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'criterion': 'gini', 'max_depth': 2, 'max_features': 'sqrt'}\n"]},{"output_type":"execute_result","data":{"text/plain":["0.6728221481085306"]},"metadata":{},"execution_count":29}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","\n","# === Su código empieza acá ===\n","# definir la grid search o random search  (grid) y entrenarlo\n","param_distributions = {'criterion': ['gini', 'entropy'],\n","                    'max_depth': [None, 2,3,4,5],\n","                    'max_features': [5, 'sqrt']\n","                    }\n","\n","classif_tree = DecisionTreeClassifier(random_state = RANDOM_STATE)\n","grid = GridSearchCV(classif_tree, param_distributions, cv = 10, scoring = scoring)\n","\n","# === Su código termina acá ===\n","grid.fit(X_train_fill_num_selected, y_train)\n","\n","print(grid.best_params_)\n","grid.best_score_"]},{"cell_type":"markdown","metadata":{"id":"93GHxfCdHVgt"},"source":["# 6: pipeline\n","Compactar todos los pasos ejecutados hasta ahora en un mismo Pipeline."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1695824459902,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"qib2hHQDHIJ0","outputId":"57ad7fc4-3fdc-4bff-ef87-2bb21674b0e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["score obtenido: 0.673 ± 0.027 %\n"]}],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import cross_validate\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","\n","# === Su código empieza acá ===\n","# definir la pipeline (pipe) y complete con la metrica seleccionada\n","transformers = [\n","        (\"ord\", OrdinalEncoder(), [0]),\n","        (\"one_hot\", OneHotEncoder(), [1, 2]),\n","        (\"scaler\", MinMaxScaler(), [3])\n","        ]\n","\n","pipe = Pipeline(\n","    [('imputer', SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')),\n","    ('transformer', ColumnTransformer(transformers = transformers, remainder = 'passthrough')),\n","    ('selector', SelectKBest(f_classif, k = 5)),\n","    ('classifier', DecisionTreeClassifier(criterion = 'gini', max_depth = 2, max_features = 'sqrt', random_state = RANDOM_STATE))\n","    ]\n",")\n","\n","result = cross_validate(pipe, X_train, y_train, cv=10, scoring=scoring)\n","# === Su código termina acá ===\n","\n","\n","score_mean = result['test_score'].mean()\n","score_std = result['test_score'].std()\n","\n","print(f'score obtenido: {score_mean:.3f} ± {score_std:.3f} %')"]},{"cell_type":"markdown","metadata":{"id":"HRGI1jD9KQ6X"},"source":["# 7: Obtener el mejor clasificador posible\n","\n","Ahora que todos los pasos estan dentro de un pipeline, podemos re ver las desiciones tomadas en cada paso para obtener el mejor clasificador posible.\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"-JUbciTi_lWl","executionInfo":{"status":"ok","timestamp":1695824459902,"user_tz":180,"elapsed":4,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}}},"outputs":[],"source":["# Definición de algoritmos de transformación para el step \"transformers\"\n","# Se evalúan opciones con y sin normalización de la columna de age, así como de\n","# codificación nominal y ordinal para la variable de pclass.\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","transformers_1 = [\n","        (\"ord\", OrdinalEncoder(), [0]),\n","        (\"one_hot\", OneHotEncoder(), [1, 2]),\n","        (\"scaler\", MinMaxScaler(), [3])\n","        ]\n","\n","transformers_2 = [\n","        (\"ord\", OrdinalEncoder(), [0]),\n","        (\"one_hot\", OneHotEncoder(), [1, 2])\n","        ]\n","\n","transformers_3 = [\n","        (\"one_hot\", OneHotEncoder(), [0, 1, 2]),\n","        (\"scaler\", MinMaxScaler(), [3])\n","        ]\n","\n","transformers_4 = [\n","        (\"one_hot\", OneHotEncoder(), [0, 1, 2])\n","        ]\n","\n","# Listado para pasar como parámetros en grid search\n","transformers = [transformers_1,transformers_2, transformers_3, transformers_4]"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"QY_lAf3cJ6iT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695824587728,"user_tz":180,"elapsed":127830,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"b80dcc8d-ab23-4a60-98b4-ab11b2ab810b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'transformer__transformers': [('ord', OrdinalEncoder(), [0]), ('one_hot', OneHotEncoder(), [1, 2])], 'selector__score_func': <function chi2 at 0x78a1475653f0>, 'selector__k': 6, 'classifier__kernel': 'rbf', 'classifier__class_weight': None, 'classifier__C': 5, 'classifier': SVC(C=5)}\n"]},{"output_type":"execute_result","data":{"text/plain":["0.6912228135956948"]},"metadata":{},"execution_count":32}],"source":["from sklearn.metrics import classification_report\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.feature_selection import RFE, SelectKBest, chi2, SequentialFeatureSelector, f_classif, mutual_info_classif\n","\n","# === Su código empieza acá ===\n","# de ser necesario, agreguen más celdas\n","# de ser necesario, importen otros modulos\n","# Si quieren, aprovechen a rever todas las decisiones tomadas hasta acá\n","\n","pipe = Pipeline(\n","    [('imputer', SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')),\n","    ('transformer', ColumnTransformer(transformers = transformers_1, remainder = 'passthrough')),\n","    ('selector', SelectKBest()),\n","    ('classifier', DecisionTreeClassifier(criterion = 'gini', max_depth = 2, max_features = 5))\n","    ])\n","\n","params = [\n","            {# KNN\n","        'transformer__transformers': transformers,\n","        'selector__score_func':[f_classif, mutual_info_classif, chi2],\n","        'selector__k': [4,5,6, 'all'],\n","        'classifier': [KNeighborsClassifier()],\n","        'classifier__n_neighbors': [5,10,15,20,25,30],\n","        'classifier__weights': ['uniform', 'distance']\n","        },\n","\n","            # Random Forest\n","        {'transformer__transformers': transformers,\n","        'selector__score_func':[f_classif, mutual_info_classif, chi2],\n","        'selector__k': [4,5,6, 'all'],\n","        'classifier': [RandomForestClassifier()],\n","        'classifier__n_estimators': [10, 50, 100, 200],\n","        'classifier__criterion': ['gini', 'entropy', 'log_loss'],\n","        'classifier__max_features': ['sqrt', 'log2', None]\n","        },\n","\n","            # Support Vector Classifier\n","        {'transformer__transformers': transformers,\n","        'selector__score_func':[f_classif, mutual_info_classif, chi2],\n","        'selector__k': [4,5,6, 'all'],\n","        'classifier': [SVC()],\n","        'classifier__C': [1,2,3,4,5,10,20],\n","        'classifier__kernel': ['poly', 'rbf'],\n","        'classifier__class_weight': ['balanced', None]\n","        },\n","\n","            # Logistic Regression\n","        {'transformer__transformers': transformers,\n","        'selector__score_func':[f_classif, mutual_info_classif, chi2],\n","        'selector__k': [4,5,6, 'all'],\n","        'classifier': [LogisticRegression()],\n","        'classifier__penalty': ['l2',None],\n","        'classifier__class_weight': ['balanced', None]\n","        },\n","\n","            # Gradient Boosting Classifier\n","        {'transformer__transformers': transformers,\n","        'selector__score_func':[f_classif, mutual_info_classif, chi2],\n","        'selector__k': [4,5,6, 'all'],\n","        'classifier': [GradientBoostingClassifier()],\n","        'classifier__loss': ['exponential', 'log_loss'],\n","        'classifier__max_features': ['sqrt', 'log2', None]\n","        }\n","    ]\n","\n","# En primer lugar, se realiza una búsqueda de hiperparámetros con RandomizedSearchCV(), de manera de identificar el clasificador con mejor\n","# métrica. Se fija el número de iteraciones en 100 de manera de probar con una cantidad razonable de combinaciones para cada algoritmo sin aumentar demasiado el costo.\n","# Luego, se realizará una búsqueda exhaustiva con GridSearchCV buscando optimizar los parámetros de ese algoritmo.\n","\n","pipe_grid = RandomizedSearchCV(pipe, params, cv = 10, scoring = scoring, error_score = 'raise', n_iter = 100, random_state = RANDOM_STATE)\n","pipe_grid.fit(X_train, y_train)\n","\n","print(pipe_grid.best_params_)\n","pipe_grid.best_score_\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nq_GCpFQ_lWl","executionInfo":{"status":"ok","timestamp":1695824642191,"user_tz":180,"elapsed":54470,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"}},"outputId":"e38c3474-85e1-460b-c0f3-0f6c84fb5e3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'classifier': SVC(C=10), 'classifier__C': 10, 'classifier__class_weight': None, 'classifier__kernel': 'rbf', 'selector__k': 6, 'selector__score_func': <function chi2 at 0x78a1475653f0>, 'transformer__transformers': [('ord', OrdinalEncoder(), [0]), ('one_hot', OneHotEncoder(), [1, 2])]}\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7025605725973374"]},"metadata":{},"execution_count":33}],"source":["# Support Vector Classifier\n","\n","params_opt = {\n","        'transformer__transformers': [transformers_1, transformers_2],\n","        'selector__score_func':[chi2],\n","        'selector__k': [6, 'all'],\n","        'classifier': [SVC()],\n","        'classifier__C': [1,2,3,4,5,10,20],\n","        'classifier__kernel': ['poly', 'rbf'],\n","        'classifier__class_weight': ['balanced', None]\n","}\n","\n","pipe_grid_opt = GridSearchCV(pipe, params_opt, cv = 10, scoring = scoring, error_score = 'raise')\n","\n","pipe_grid_opt.fit(X_train, y_train)\n","\n","print(pipe_grid_opt.best_params_)\n","pipe_grid_opt.best_score_"]},{"cell_type":"markdown","metadata":{"id":"vEvlfuYASElX"},"source":["# 8: evaluacion\n","\n","\n","Una vez encontrado este clasificador, evaluarlo sobre el dataset de test con la funcion `sklearn.metrics.classification_report`"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695824642191,"user":{"displayName":"Diego Velasco","userId":"01201883113506685343"},"user_tz":180},"id":"HJqeS7bHNqFg","outputId":"a7333f1e-d77d-4e87-8c4d-67ea4420a601"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.87      0.81      0.84       264\n","           1       0.66      0.75      0.70       130\n","\n","    accuracy                           0.79       394\n","   macro avg       0.76      0.78      0.77       394\n","weighted avg       0.80      0.79      0.79       394\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","# === Su código empieza acá ===\n","# Utilizar el mejor modelo encontrado para clasificar X_test\n","# Asegurate de que este entrenado con los datos correctos\n","\n","y_pred = pipe_grid_opt.predict(X_test)\n","# === Su código termina acá ===\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"nBit7JXfoCLi"},"source":["# Pregunta E\n","Suponiendo que el resultado de la celda anterior es este:\n","```python\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.81      0.82       264\n","           1       0.64      0.68      0.66       130\n","\n","    accuracy                           0.77       394\n","   macro avg       0.74      0.75      0.74       394\n","weighted avg       0.77      0.77      0.77       394\n","```\n","\n","Interprete con sus palabras, para una persona no tecnica, la implicancia de obtener:\n","\n","- precision = 0.84 para la clase 0\n","- recall = 0.68 para la clase 1\n","\n","**Respuesta**\n","1. Un valor de precision de 0.84 para la clase 0 implica que, de todos los pasajeros del conjunto de testeo para los cuales se predijo que el pasajero no sobrevivió, se predijo correctamente al 84%.\n","2. Un valor de recall de 0.68 para la clase 1 implica que, del total de pasajeros del conjunto de testeo que efectivamente sobrevivieron, se clasificó correctamente al 68%."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}